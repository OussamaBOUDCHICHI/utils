# -*- coding: utf-8 -*-
"""ComputingAssignment-MFD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EwYZbM0UpMFyKtrlyR8OvRcDXJovDNLP

### Volatility Modeling, Master Mathematics for Finance and Data (MFD), Ecole des Ponts ParisTech and Universit√© Gustave Eiffel, 2022-23
# Computing assignment

### Due Date: 1:30 PM Monday, February 27
You should turn in the notebook at the Educnet website.

Please comment your code properly.

Before you turn in the notebook, press the "Run all cells" button in the toolbar, and make sure all the calculation results and graphs are produced correctly in a reasonable time frame, and then save the notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import interp1d
plt.rc('figure', figsize=(6, 5.5))

"""# 1. Conditional Expection and Least Square Regression

Let $X$ and $Y$ be two random variables. The conditional expectation $\mathbb{E}\left[Y|X\right]$ is a function $f^{\ast}$ of $X$ that best approximates $Y$ in the least square sense, i.e.,

$$\mathbb{E}\left[Y|X\right]=f^{\ast}(X)\quad\text{and}\quad\mathbb{E}\left[\left|Y-f^{\ast}(X)\right|^2\right]\leq\mathbb{E}\left[\left|Y-f(X)\right|^2\right]\text{ for any function }f\text{ of }X$$

Let us define the random variables $X$ and $Y$ by
$$
g(x) = x \frac{1 + x}{1 + x^2}, \qquad X \sim \mathcal{N}(0, 1), \quad Y = g(X) + \varepsilon
$$
where $\varepsilon \sim \mathcal{N}(0, 1/16)$ is independent of $X$.

Note that $\mathbb{E}\left[\left.Y\right|X\right] = \mathbb{E}\left[\left.g(X)+\varepsilon\right|X\right] = g(X)+\mathbb{E}\left[\left.\varepsilon\right|X\right] = g(X)$.
"""

# Plot a random sample of the joint distribution and the 
# theoretical conditional expectation of Y wrt X.

def g(x):
    return x*(1+x)/(1+x**2)

n = 200
sigma = 0.25
X = np.random.randn(n)
Y = g(X) + sigma * np.random.randn(n)

fig, ax = plt.subplots()
ax.scatter(X, Y, alpha=0.5)
x = np.linspace(X.min(), X.max(), 101)
ax.plot(x, g(x), 'k', linewidth=2)
ax.set_xlabel('X')
ax.set_ylabel('Y')

"""## Parametric regression

The conditional expection $\mathbb{E}\left[\left.Y\right|X\right]$ is approximated by a linear combination of a set of given <em>basis</em> functions $\{f_i(X)\}_{0\leq i\leq n}$, i.e.,

$$\mathbb{E}\left[Y|X\right]\approx \beta_0^*f_0(X)+\cdots+\beta_n^*f_n(X)$$

where 

$$\mathbb{E}\left[\left(Y-\beta_0^*f_0(X)-\cdots-\beta_n^*f_n(X)\right)^2\right] = \min_{\beta_1,\ldots,\beta_n}\mathbb{E}\left[\left(Y-\beta_0f_0(X)-\cdots-\beta_nf_n(X)\right)^2\right]$$

Given $N$ observations $\left((x_1,y_1),\ldots,(x_N, y_N)\right)$ of $X$ and $Y$, one finds the optimal parameters $\beta_i$ by solving the least square problem $\min_\beta\|A\beta-y\|_2$, where

$$A=\begin{bmatrix}f_0(x_1) & \cdots & f_n(x_1)\\ \vdots & \ddots & \vdots \\ f_0(x_N) & \cdots & f_n(x_N)\end{bmatrix}\quad
\text{and}\quad y=\begin{bmatrix}y_1\\ \vdots \\ y_N\end{bmatrix}$$

The numpy routine <strong>numpy.linalg.lstsq</strong> can be used to solve such linear least square problems.

### Polynomials

The basis functions are taken to be power functions $f_i(X)=X^{i}$, that is $\mathbb{E}\left[Y|X\right]$ is a polynomial of $X$. The numpy routine <strong>numpy.polyfit</strong> is a convenient way to obtain the least square polynomial fit.
"""

# Fit a polynomial of degree 3 to the sample points (X, Y)
p = np.polyfit(X, Y, deg=3)
fig, ax = plt.subplots()
ax.scatter(X, Y, alpha=0.5)
ax.plot(x, np.polyval(p, x), 'r', lw=2, label='Polynomial Fit')
ax.plot(x, g(x), 'k', label='True Function')
ax.legend(loc=0)

"""### Piecewise Linear Regression"""

def pwlin_basis(xknots):
    """Basis that represent a piecewise linear function with given knots"""
    fs = [lambda x: np.ones_like(x, dtype=float), lambda x: x-xknots[0]]
    fs.extend([lambda x, a=xknots[i]: np.maximum(x-a, 0) for i in range(len(xknots))])
    return fs

def pwlin_fit(xdata, ydata, xknots):
    """Fit a piecewise linear function with xknots to xdata and ydata"""
    fs = pwlin_basis(xknots)
    A = np.column_stack([f(xdata) for f in fs])
    ps = np.linalg.lstsq(A, ydata, rcond=None)[0]
    return ps, fs

xknots = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), 8)
ps, fs = pwlin_fit(X, Y, xknots)
fig, ax = plt.subplots()
ax.scatter(X, Y, alpha=0.5)
ax.plot(x, sum([f(x)*p for (f, p) in zip(fs, ps)]), 'r', label='Piecewise Linear Fit')
ax.plot(x, g(x), 'k', label='True Function')
ax.legend(loc=0)
ax.set_xlabel('X')
ax.set_ylabel('Y')

"""## Nonparametric regression

### Nadaraya-Watson Kernel regression (Local Weighted Average)

Here, no parametric form is assumed for $\mathbb{E}[Y|X]$. Instead, local averages of $Y$ values are computed, given the value of $X$:

$$\mathbb{E}\left[Y|X=x\right]\approx\frac{\sum_{i=1}^NK_h(x-x_i)y_i}{\sum_{i=1}^NK_h(x-x_i)}$$
where $K$ is a kernel function and $K_h(x)=K(x/h)/h$, $h$ is the <em>bandwidth</em>. $K_h$ approximates the Dirac mass at zero.

### Local Linear Regression

The locally weighted linear regression solves a separate weighted least squares problem at each target point $x$,

$$\hat{\alpha},\hat{\beta} = \text{argmin}_{\alpha,\beta}\sum_{i=1}^NK_h(x-x_i)\left[y_i-\alpha-\beta x_i\right]^2$$

which yields an estimate $\hat{\alpha}+\hat{\beta}x$. Note that $\hat{\alpha}$ and $\hat{\beta}$ depend on $x$. The locally-weighted averages can be badly biased on the boundaries. This bias
can be removed by local linear regression to the first order.

<b>Note.</b> To speed up, we often perform the local regression only at a selection of points and then use interpolation/extrapolation to evaluate at other target points.
"""

# Non-parametric regression function

def gauss_kern(x):
    """Gaussian kernel function"""
    return np.exp(-x**2/2)

def kern_reg(x, xdata, ydata, bandwidth, kern=gauss_kern):
    """Nadaraya-Watson Kernel Regression (Locally weighted average)

    Parameters
    ----------
    x: array_like, one-dimensional
        The x-coordinates of the target points
    xdata: array_like
        The x-coordinates of the data points.
    ydata: array_like
        The y-coordinates of the data points. 
    bandwidth: positive scalar
        Bandwidth of the kernel
    kern: callable
        kernel function
    """
    weights = kern((xdata[:, np.newaxis] - x) / bandwidth)
    return np.sum(weights * ydata[:, np.newaxis], axis=0) / np.sum(weights, axis=0)


def ll_reg(x, xdata, ydata, bandwidth, kern=gauss_kern):
    """Local Linear Regression

    Parameters
    ----------
    x: array_like, one-dimensional
        The x-coordinates of the target points
    xdata: array_like
        The x-coordinates of the data points.
    ydata: array_like
        The y-coordinates of the data points. 
    bandwidth: positive scalar
        Bandwidth of the kernel
    kern: callable
        kernel function
    """
    
    def func(xx):
        weights = np.sqrt(kern((xdata-xx)/bandwidth))
        b = ydata*weights
        A = np.column_stack((np.ones_like(xdata), xdata-xx))*weights[:, np.newaxis]
        yy, _ = np.linalg.lstsq(A, b, rcond=None)[0]
        return yy
    
    return np.vectorize(func)(x)

fig, axs = plt.subplots(1,2, figsize=(11, 4.75))
bw_silverman = (4/(3*len(X)))**0.2*np.std(X)
# NW Kernel Regression
xknots0 = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), 20)
yknots0 = kern_reg(xknots0, X, Y, bw_silverman, gauss_kern)
f0 = interp1d(xknots0, yknots0, kind='linear', fill_value='extrapolate')
axs[0].plot(x, f0(x), color='r', label='NW Kernel Reg (bw={:.2f})'.format(bw_silverman))
axs[0].set_title('NW Kernel Regression')
# Local Linear Regression
xknots1 = xknots0
yknots1 = ll_reg(xknots1, X, Y, bw_silverman, gauss_kern)
f1 = interp1d(xknots1, yknots1, kind='linear', fill_value='extrapolate')
axs[1].plot(x, f1(x), color='r', label='Local Linear Reg (bw={:.2f})'.format(bw_silverman))
axs[1].set_title('Local Linear Regression')
for ax in axs:
    ax.scatter(X, Y, alpha=0.5)
    ax.plot(x, g(x), 'k', label='True Function')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.legend(loc=0)

"""## Questions

<b>(a)</b>. (Parametric regression) In general, increasing the number of basis functions in the regression gives us more flexibility to better fit the data. However, having too many parameters in the model oftentimes leads to overfitting, which usually has poor predictive performance and is over-sensitive to small noise in the data. To observe the overftting phenomenon, in polynomial fit, try to use different degrees of the polynomials; in piecewise-linear regression, try to use different numbers of knots. Then reproduce the scatter plot with fitted regression function. Compare and comment on the results.
"""

#Polynomial fit of degree 5, 10, 15 to the sample points (X, Y)
fig, axs = plt.subplots(1,3, figsize=(18, 5))

deg = [5, 10, 15]
for i in range(3):
    p = np.polyfit(X, Y, deg[i])
    axs[i].plot(x, np.polyval(p, x), 'r', lw=2, label='Polynomial Fit of degree {:.0f} '.format(deg[i]))

for ax in axs:
    ax.scatter(X, Y, alpha=0.5)
    ax.plot(x, g(x), 'k', label='True Function')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.legend(loc=0)

# Piecewise linear regression with 10, 20, 30 as number of knots
fig, axs = plt.subplots(1,3, figsize=(18, 5))
nb_knots = [10, 20, 30]
for i in range(3):
    xknots = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), nb_knots[i])
    ps, fs = pwlin_fit(X, Y, xknots)
    axs[i].plot(x, sum([f(x)*p for (f, p) in zip(fs, ps)]), 'r', label='Piecewise Linear Fit (nb_knots={:.0f})'.format(nb_knots[i]))

for ax in axs:
    ax.scatter(X, Y, alpha=0.5)
    ax.plot(x, g(x), 'k', label='True Function')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.legend(loc=0)

"""<b>(b)</b>. For nonparametric regression,
<ul>
<li>Try different bandwidth values in the kernel regression. Reproduce the scatter plot with fitted regression function. Compare and comment on the results. For what values of the bandwidth do we observe overfitting? For what values of the bandwidth do we observe a poor fit?</li>
<li>Try to use different kernels, for example
$$K(x)=(x+1)^2(1-x)^2\quad\text{for }-1\leq x\leq 1\quad\text{and}\quad0\quad\text{ elsewhere.}$$
Which has more impact: the bandwidth $h$ or the kernel $K$?
</ul>
"""

def K(x):
  return ((x + 1.) ** 2) * ((1. - x)**2) * ((x <= 1.) & (x >= -1.))

X = np.linspace(0, 2, 10)

np.array([K(y) for y in X])

fig, axs = plt.subplots(2,2, figsize=(12, 12))
bw = [0.1, 0.3, 0.5, 1]
# NW Kernel Regression
xknots0 = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), 20)
fct = [] 
for i in range(4):
    yknots0 = kern_reg(xknots0, X, Y, bw[i], gauss_kern)
    f0 = interp1d(xknots0, yknots0, kind='linear', fill_value='extrapolate')
    fct.append(f0)


axs[i, j].plot(x, fct[0](x), color='r', label='NW Kernel Reg (bw={:.2f})'.format(bw[0]))
axs[0, 1].plot(x, fct[1](x), color='r', label='NW Kernel Reg (bw={:.2f})'.format(bw[1]))
axs[1, 0].plot(x, fct[2](x), color='r', label='NW Kernel Reg (bw={:.2f})'.format(bw[2]))
axs[1, 1].plot(x, fct[3](x), color='r', label='NW Kernel Reg (bw={:.2f})'.format(bw[3]))

for ax in axs.flat:
    ax.scatter(X, Y, alpha=0.5)
    ax.plot(x, g(x), 'k', label='True Function')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_title('NW Kernel Regression')
    ax.legend(loc=0)

fig, axs = plt.subplots(3,3, constrained_layout=True)
idx = [-1, 6, 13, -5, 9, 7]
for i in range(3):
    for j in range(3):
        axs[i,j].plot(fitted[TT[idx[i + j]]]['x'], fitted[TT[idx[i + j]]]['w'],color='darkred')
        axs[i,j].scatter(Test[temps[idx[i+j]]]['x'], Test[temps[idx[i+j]]]['w'], facecolors='darkblue', edgecolors='k', label='Market')
        axs[i,j].set_title('T = ' + str(TT[idx[i+j]])[0:5], fontdict={'weight':'bold'})
        axs[i,j].legend(prop={'weight':'bold'})
        axs[i,j].set_xlabel('x', fontdict={'weight':'bold'})
        axs[i,j].set_ylabel('w(x)', fontdict={'weight':'bold'})

# def Kern(x, type = 'example'):
#     W = []
#     for i in range(len(x)):
#         if np.abs(x[i]) <= 1:
#             if type == 'example':
#                 W.append((x[i] + 1)**2 * (1 - x[i])**2)
#             if type == 'tricube':
#                 W.append((1 - np.abs(x[i])**3)**3)
#             if type == 'Epanechnikov':
#                 W.append(3/4 * (1 - x[i]**2))
#             if type == 'Triangular':
#                 W.append((1 - np.abs(x)))
#         else:
#             W.append(0)

def Kern_1(x):
    W = []
    for i in range(len(x)):
        if (-1 <= x[i]) and (x[i] <= 1):
            W.append((x[i] + 1)**2 * (1 - x[i])**2)
        else:
            W.append(0)
    return W

# Tricube
def Kern_2(x):
    W = []
    for i in range(len(x)):
        if (-1 <= x[i]) and (x[i] <= 1):
            W.append((1 - np.abs(x[i])**3)**3)
        else:
            W.append(0)
    return W

# Epanechnikov
def Kern_3(x):
    W = []
    for i in range(len(x)):
        if (-1 <= x[i]) and (x[i] <= 1):
            W.append(3/4 * (1 - x[i]**2))
        else:
            W.append(0)
    return W

# # Triangular
# def Kern_4(x):
#     W = []
#     for i in range(len(x)):
#         if (-1 <= x[i]) and (x[i] <= 1):
#             W.append((1 - np.abs(x)))
#         else:
#             W.append(0)
#     return W

fig, axs = plt.subplots(1,3, figsize=(18, 5))
bw_silverman = (4/(3*len(X)))**0.2*np.std(X)
xknots1 = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), 20)
Kernels = [Kern_1, Kern_2, Kern_3]
fct1 = []
for i in range(3):
    yknots1 = ll_reg(xknots1, X, Y, bw_silverman, Kernels[i])
    f1 = interp1d(xknots1, yknots1, kind='linear', fill_value='extrapolate')
    fct1.append(f1)
    axs[i].plot(x, fct1[i](x), color='r', label='Local Linear Reg (Kern_{:.0f})'.format(i+1))
for ax in axs:
    ax.scatter(X, Y, alpha=0.5)
    ax.plot(x, g(x), 'k', label='True Function')
    ax.set_title('Local Linear Regression')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.legend(loc=0)

"""# 2. The Particle Method and Smile Calibration

<!--<h3 style="color:deepskyblue">The Particle Method and Smile Calibration</h3>-->

Consider the stochastic local volatility (SLV) model

$$
\begin{array}{l}
dS_t = a_t l(t, S_t) S_t dW^{(1)}_t\\
a_t=\sigma_0 e^{Y_t}\quad\text{where}\quad d Y_t = -\kappa Y_tdt+\gamma dW^{(2)}_t\\
d \langle W^{(1)}, W^{(2)} \rangle_t = \rho dt.
\end{array}
$$

The numerical values for the model parameters are
- $T = 1$.
- $S_0 = 100$.
- $\sigma_0 = 15\%$.
- $Y_0 = 0$.
- $\rho = -50\%$.
- $\gamma = 50\%$.
- $\kappa = 1$.

The goal is to find a leverage function $l(t, S)$ so that this model matches the market prices of vanilla options. For the sake of simplicity, we assume that the market implied volatility surface is flat $\sigma_{\textrm{Market}} \equiv 15\%$. In that case, we also have $\sigma_{\textrm{loc}}(t,S) \equiv 15\%$.

Below we describe a Monte Carlo simulation scheme for the SLV model.

First we discretize the interval $(0,T)$ into subintervals $(t_{i-1}, t_i)$, $1\leq i\leq n$, and set $\Delta t_i=t_i-t_{i-1}$. 

The Ornstein-Uhlenbeck process $Y$ is explicitly solvable:
\begin{equation}
Y_{t_i}=e^{-\kappa\Delta t_i}Y_{t_{i-1}}+\int_{t_{i-1}}^{t_i}\gamma e^{-\kappa\left(t_i-s\right)}dW_s^{(2)}.
\end{equation}

Therefore for given $Y_{t_{i-1}}$, $Y_{t_i}$ is a Gaussian variable with 
\begin{equation}\tag{1}
\mathbb{E}\left[\left.Y_{t_i}\right\vert Y_{t_{i-1}}\right]=e^{-\kappa\Delta t_i}Y_{t_{i-1}},\quad
\text{var}\left[\left.Y_{t_i}\right\vert Y_{t_{i-1}}\right]=\frac{\gamma^2}{2\kappa}\left(1-e^{-2\kappa\Delta t_i}\right)
\end{equation}

so that paths of $Y$ can be simulated exactly.

To simualte the spot process $S$, we use the Euler scheme:
$$\log S_{t_i}-\log S_{t_{i-1}}=-\frac{1}{2}\sigma_0^2e^{2Y_{t_{i-1}}}l(t_{i-1},S_{t_{i-1}})^2\Delta t_i+\sigma_0 e^{Y_{t_{i-1}}}l(t_{i-1},S_{t_{i-1}})\int_{t_{i-1}}^{t_i}dW^{(1)}_t$$

Thus for given $S_{t_{i-1}}$ and $Y_{t_{i-1}}$, $\log S_{t_i}$ and $Y_{t_i}$ are jointly Gaussian variables with

\begin{equation}\tag{2}
\mathbb{E}\left[\left.\log S_{t_i}\right\vert S_{t_{i-1}},Y_{t_{i-1}}\right]=\log S_{t_{i-1}}-\frac{1}{2}\sigma_0^2e^{2Y_{t_{i-1}}}\Delta t_i,\quad\text{Var}\left[\left.\log S_{t_i}\right\vert S_{i_{i-1}}, Y_{t_{i-1}}\right]=\sigma_0^2e^{2Y_{t_{i-1}}}l(t_{i-1},S_{t_{i-1}})^2\Delta t_i
\end{equation}

\begin{equation}\tag{3}
\text{Cov}\left[\left.Y_{t_i},\log S_{t_i}\right\vert S_{t_{i-1}}, Y_{t_{i-1}}\right]=\sigma_0 e^{Y_{t_{i-1}}}l(t_{i-1},S_{t_{i-1}})\frac{\gamma\rho}{\kappa}\left(1-e^{-\kappa\Delta t_i}\right)
\quad\text{or}\quad
\text{Corr}\left[\left.Y_{t_i},\log S_{t_i}\right\vert S_{t_{i-1}}, Y_{t_{i-1}}\right]=\rho\sqrt{\frac{2(1-e^{-\kappa\Delta t_i})}{\kappa\Delta t_i(1+e^{-\kappa\Delta t_i})}}
\end{equation}

Over the time interval $[t_{i-1}, t_i]$, we can advance $\log S$ and $Y$ by generating Gaussian variables $\log S_{t_i}$ and $Y_{t_i}$ using equations (1)-(3). That is,

\begin{align}
\log S_{t_i} &= \log S_{t_{i-1}}-\frac{1}{2}\sigma_0^2e^{2Y_{t_{i-1}}}l(t_{i-1},S_{t_{i-1}})^2\Delta t_i+\sigma_0e^{Y_{t_{i-1}}}l(t_{i-1},S_{t_{i-1}})\sqrt{\Delta t_i}\left(\sqrt{1-\bar{\rho}^2}Z_1+\bar{\rho}Z_2\right),\quad\text{where }\bar{\rho}=\rho\sqrt{\frac{2(1-e^{-\kappa\Delta t_i})}{\kappa\Delta t_i(1+e^{-\kappa\Delta t_i})}}\\
Y_{t_i} &= e^{-\kappa\Delta t_i}Y_{t_{i-1}}+\gamma\sqrt{\frac{1-e^{-2\kappa\Delta t_i}}{2\kappa}}Z_2
\end{align}
where $Z_1$ and $Z_2$ are independent standard normal variables.
"""

T, s_0, sig_0, y_0, rho, gamma, kappa = 1., 100., .15, 0., -.5, .5, 1.
sig_loc = .15

n_steps = 100
N = 10_000
dt = 1. / n_steps
n_xgrid = 30
leverage = np.zeros((n_steps, 30))

from scipy import stats

K = lambda x: ((x + 1.) ** 2) * ((1. - x)**2) * ((x <= 1.) & (x >= -1.))

def kern_reg(x, x_data, y_data, bw, kern = K):
  weights = np.array([K((x - x_i) / bw) / bw for x_i in x_data])

  return np.dot(weights, y_data) / weights.sum()

rho_bar = rho * np.sqrt(2. * (1. - np.exp(-kappa * dt)) / (kappa * dt * (1. + np.exp(-kappa * dt))))

np.random.seed(0)

rng = lambda n, d=2: np.random.randn(n, d)

q_ln = lambda q, k:  stats.lognorm.ppf(q, loc = s_0, s = sig_loc * np.sqrt(k * dt))

from scipy.interpolate import CubicSpline

def generate_xgrid(x_min, x_max, n_points, method = 'linear'):
  assert((method == 'linear') | (method == 'random'))
  
  x_grid = None
  if method == 'linear': x_grid = np.linspace(x_min, x_max, n_points)
  else: 
    x_grid = x_min + (x_max - x_min) * np.random.rand(n_points)
    x_grid = np.sort(x_grid)
  
  return x_grid

# Example:
x_min, x_max = q_ln(.05, 1), q_ln(.95, 1)

n_xgrid = 50
x_grid_lin = generate_xgrid(x_min, x_max, n_xgrid)
x_grid_rdm = generate_xgrid(x_min, x_max, n_xgrid, 'random')

import matplotlib as mpl

plt.rcParams['figure.figsize'] = (10, 8)
mpl.rcParams['axes.linewidth'] = 2

plt.plot(x_grid_lin, [2.] * n_xgrid, ".", color = 'firebrick')
plt.plot(x_grid_rdm, [2.] * n_xgrid, "x", color = 'navy', alpha = .7)
plt.ylim([1.95, 2.06])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# y = y_0 * np.ones(N)
# ls = np.log(s_0) * np.ones(N)
# leverage[0, :] = (sig_loc / (sig_0 * np.exp(y_0))) * np.ones(30)
# x_grid = np.zeros((n_steps, n_xgrid))
# x_grid[0, :] = s_0 * np.ones(30)
# 
# for i in range(1, n_steps):
#   z = rng(N)
#   z_1, z_2 = z[:, 0], z[:, 1]
#   if i == 1:
#     lev_inter = lambda x: leverage[0, 0]
#   else:
#     lev_inter = interp1d(x_grid[i - 1, :], leverage[i - 1, :], kind = 'linear', fill_value='extrapolate')
#   
#   ls = ls - .5 * dt * (sig_0 * np.exp(y) * lev_inter(np.exp(ls)))**2  + sig_0 * np.exp(y) * lev_inter(np.exp(ls)) * np.sqrt(dt) * (np.sqrt(1 - rho_bar**2) * z_1 + rho_bar * z_2)
#   s = np.exp(ls)
#   y = y * np.exp(-kappa * dt) + gamma * np.sqrt((1. - np.exp(-2. * kappa * dt)) / (2. * kappa)) * z_2
#   
#   a = sig_0 * np.exp(y)
#   bw = kappa * sig_loc * s_0 * np.sqrt(np.maximum(i * dt, .15)) / (N**(.2))
#   x_min, x_max = q_ln(.05, i), q_ln(.95, i)
#   x_grid[i, :] = np.linspace(x_min, x_max, n_xgrid)
#   leverage[i, :] = sig_loc / np.array([np.sqrt(kern_reg(x, s, a**2, bw)) for x in x_grid[i, :]])
#

"""**See this code (interesting) : [code_chataign](https://github.com/mChataign/Beyond-Surrogate-Modeling-Learning-the-Local-Volatility-Via-Shape-Constraints)**"""

import matplotlib.pyplot as plt
import matplotlib as mpl

plt.rcParams['figure.figsize'] = (10, 8)
mpl.rcParams['axes.linewidth'] = 2

plt.plot(x_grid[1, :], leverage[1, :], '.--', color = 'navy')

N_2 = 1_000
strikes = np.array([70., 80., 90., 100., 110., 120., 130., 140.])
Z = np.random.randn(N_2, n_steps - 1, 2)

def get_price(K):
  m = 0.
  
  for j in range(N_2):
    y = y_0
    ls = np.log(s_0)
    #z = rng(n_steps - 1)
    
    for i in range(1, n_steps):
      #z = rng()
      z_1, z_2 = Z[j, i - 1, :]
      
      if i == 1:
         lev_inter = lambda x: leverage[0, 0]
      else:
         lev_inter = interp1d(x_grid[i - 1, :], leverage[i - 1, :], kind='linear', fill_value='extrapolate')
      
      ls = ls - .5 * dt * (sig_0 * np.exp(y) * lev_inter(np.exp(ls)))**2  + sig_0 * np.exp(y) * lev_inter(np.exp(ls)) * np.sqrt(dt) * (np.sqrt(1 - rho_bar**2) * z_1 + rho_bar * z_2)
      y = y * np.exp(-kappa * dt) + gamma * np.sqrt((1. - np.exp(-2. * kappa * dt)) / (2. * kappa)) * z_2
    
    m += np.maximum(np.exp(ls) - K, 0.) 

  return m / N_2

N = stats.norm.cdf
n = stats.norm.pdf


def PriceBS(S, t, K, r, T, sig, flag='C'):

    d_1  = (np.log(S/K) + (r+(sig**2/2))*np.sqrt(T-t))  / (sig * np.sqrt(T-t))
    d_2 = d_1 - sig * np.sqrt(T-t)
    C = S * N(d_1) - K * np.exp(-r*(T-t)) * N(d_2)
    if flag =='C':
        return C
    if flag=='P':
        return  C - S + K * np.exp(-r*(T-t))
    if flag not in ['C', 'P']:
        raise ValueError('Enter C or P')  


# Greeks :

class Greeks:
    
    def __init__(self, S, t, K, r, T, sig):
        self.S = S
        self.t = t
        self.K = K
        self.r = r
        self.T = T
        self.sig = sig
        return None

    def __d_1(self):
        
        d_1  = (np.log(self.S / self.K) + (self.r+(self.sig**2/2))*np.sqrt(self.T-self.t))  / (self.sig * np.sqrt(self.T-self.t))
        return d_1
    def __d_2(self):
        
        d_2 = Greeks.__d_1(self) - self.sig * np.sqrt(self.T-self.t)
        return d_2
    

    def Delta(self):
        
        return N(Greeks.__d_1(self))

    def Gamma(self):
        
        return n(Greeks.__d_1(self)) / (self.S * self.sig * np.sqrt(self.T-self.t))

    def Theta(self):
        
        return (self.S * self.sig * n(Greeks.__d_1(self)))/ (2 * np.sqrt(self.T - self.t)) + self.r * self.K * np.exp(-self.r * (self.T - self.t)) * N(Greeks.__d_2(self))

    def Vega(self):
        
        return self.S * np.sqrt(self.T - self.t) *    n(Greeks.__d_1(self))
    
    def Rho(self):
        
        return self.K * (self.T - self.t) *  np.exp(-self.r * (self.T - self.t)) * N(Greeks.__d_2(self))

    def dCdK(self):
        
        return - np.exp(-self.r * (self.T - self.t)) * N(Greeks.__d_2(self))    


def implidVolatility(S, t, K, r, T, sig_0, 
                     C_market, maxIter = 50, 
                     tolerance = 1e-5, 
                     method = 'N-R', 
                     flag = 'C', 
                     a = .0001, b = 2.0):
    
    if method == 'N-R':

        sig = sig_0
        C = PriceBS(S, t, K, r, T, sig, flag)

        stopping_criterion = np.abs(C - C_market)
        iter = 0

        while((stopping_criterion > tolerance) & (iter < maxIter)):
            iter += 1
            Vega = Greeks(S, t, K, r, T, sig).Vega()
            
            if Vega == float(0):
                message = 'Vega equals ', 0 , 'at iteration :', iter, '. I Suggest another method. Sigma will be put to 0.'
                sig = 0.
                break
            else :
                message = 'Algorithm Converged in : ', iter, ' iterations' 

            sig = sig - (C - C_market)  / Vega
            
            C = PriceBS(S, t, K, r, T, sig, flag)
            stopping_criterion = np.abs(C - C_market)
        
        print(message)
        return sig    

    if method == 'Dichotomy':
        C_min = PriceBS(S, t, K, r, T, a, flag)
        C_max = PriceBS(S, t,K, r, T, b, flag)

        
        try:
            assert((C_min <= C_market) & (C_market <= C_max))

        except AssertionError:
            eps = 0.1
            a = np.maximum(a - eps, 0.001)
            b = np.minimum(b + eps, 3.0)
            

        sig_min = a
        sig_max = b

        sig = (sig_min + sig_max)  / 2
        C = PriceBS(S, t, K, r, T, sig, flag)
        stopping_criterion = np.abs(C - C_market)
        iter = 0

        while((stopping_criterion > tolerance) & (iter < maxIter)):
            iter += 1

            if C - C_market > 0 :
                sig_max = sig
                sig = (sig_min + sig_max) / 2
            else :
                sig_min = sig
                sig = (sig_min + sig_max) / 2
            C = PriceBS(S, t, K, r, T, sig, flag)
            stopping_criterion = np.abs(C - C_market)

        print('Algorithm Converged in : ', iter, ' iterations.')
        return sig

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# get_price(120.)



implidVolatility(s_0, 0., 120., 0., 1., .1, 
                     0.891275925792101, tolerance = 1e-11, maxIter = 1000, method = 'Dichotomy')

PriceBS(s_0, 0., 120., .0, 1., .15)

gammas = [0., .25, .50, .75]

T, s_0, sig_0, y_0, rho, _, kappa = 1., 100., .15, 0., 0., .5, 1.
sig_loc = .15

n_steps = 100
N = 10_000
dt = 1. / n_steps
n_xgrid = 30
rho_bar = rho * np.sqrt(2. * (1. - np.exp(-kappa * dt)) / (kappa * dt * (1. + np.exp(-kappa * dt))))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# results = {}
# 
# for gamma in gammas:
#   leverage = np.zeros((n_steps, 30))
#   leverage[0, :] = (sig_loc / (sig_0 * np.exp(y_0))) * np.ones(30)
# 
#   y = y_0 * np.ones(N)
#   ls = np.log(s_0) * np.ones(N)
# 
#   x_grid = np.zeros((n_steps, n_xgrid))
#   x_grid[0, :] = s_0 * np.ones(30)
#   
#   for i in range(1, n_steps):
#     z = rng(N)
#     z_1, z_2 = z[:, 0], z[:, 1]
# 
#     if i == 1:
#       lev_inter = lambda x: leverage[0, 0]
#     else:
#       lev_inter = interp1d(x_grid[i - 1, :], leverage[i - 1, :], kind = 'linear', fill_value='extrapolate')
# 
#     ls = ls - .5 * dt * (sig_0 * np.exp(y) * lev_inter(np.exp(ls)))**2  + sig_0 * np.exp(y) * lev_inter(np.exp(ls)) * np.sqrt(dt) * (np.sqrt(1 - rho_bar**2) * z_1 + rho_bar * z_2)
#     s = np.exp(ls)
#     y = y * np.exp(-kappa * dt) + gamma * np.sqrt((1. - np.exp(-2. * kappa * dt)) / (2. * kappa)) * z_2
# 
#     a = sig_0 * np.exp(y)
#     bw = kappa * sig_loc * s_0 * np.sqrt(np.maximum(i * dt, .15)) / (N**(.2))
#     x_min, x_max = q_ln(.05, i), q_ln(.95, i)
#     x_grid[i, :] = np.linspace(x_min, x_max, n_xgrid)
#     leverage[i, :] = sig_loc / np.array([np.sqrt(kern_reg(x, s, a**2, bw)) for x in x_grid[i, :]])
#   
#   results[str(gamma)] = {'x_grid': x_grid, 'leverage': leverage}
#

mpl.rcParams.update(mpl.rcParamsDefault)

plt.rcParams['figure.figsize'] = (10, 8)
mpl.rcParams['axes.linewidth'] = 2

plt.rcParams.update({
  "text.usetex": False
})

S = np.linspace(70., 150., 500)

colors = ['firebrick', 'navy', 'purple', 'k']
for color, key in zip(colors, results.keys()):
  x, y = results[key]['x_grid'][-1, :], results[key]['leverage'][-1, :]
  f_0 = interp1d(x, y, kind = 'linear', fill_value='extrapolate')
  plt.plot(S, f_0(S), '--', color = color, label = r'$\gamma=' + str(key) + '$')

plt.xlabel(r'$S$')
plt.ylabel(r'$l(T, S)$')
plt.legend()

Ks = np.linspace(70., 200., 50)
N_2 = 10_000
Z = np.random.randn(N_2, n_steps - 1, 2)

def get_price1(K):
  m = 0.
  for j in range(N_2):
    y = y_0
    ls = np.log(s_0)
    #z = rng(n_steps - 1)
    for i in range(1, n_steps):
      #z = rng()
      z_1, z_2 = Z[j, i - 1, :]
      
      ls = ls - .5 * dt * (sig_0 * np.exp(y) )**2  + sig_0 * np.exp(y) * np.sqrt(dt) * (np.sqrt(1 - rho_bar**2) * z_1 + rho_bar * z_2)
      y = y * np.exp(-kappa * dt) + gamma * np.sqrt((1. - np.exp(-2. * kappa * dt)) / (2. * kappa)) * z_2
    
    m += np.maximum(np.exp(ls) - K, 0.) 

  return m / N_2

gamma

x = get_price1(100)

x

implidVolatility(100., 0., 100., 0., 1., .1, 7.261060742651232, tolerance = 1e-11)

IV = []
for K in Ks: 
  x = get_price1(K)
  IV.append(implidVolatility(s_0, 0., K, 0., 1., .1, x, tolerance = 1e-10, method = 'Dichotomy'))

"""**To obtain, a Non-arbitrageable imp. vol surface, one must perform an interpolation preserving the No-Arbitrage condition. See e.g** [section 2.1, p. 14 of the book](https://drive.google.com/file/d/1ekelLvlZvt23E9GCcsxh3aRrg5fx0b0m/view?usp=share_link). """

plt.plot(Ks, IV)



"""## Questions

<b>(a).</b> Implementation of the Particle Method.
- Implement the particle method studied in class to find the leverage function $l$. For this purpose, you may use the zero-th order non-parametric regression routine provided in the first exercise. We suggest that you use the quartic kernel

$$K(x)=(x+1)^2(1-x)^2\quad\text{for }-1\leq x\leq 1\quad\text{and}\quad0\quad\text{ elsewhere}$$

together with the bandwidth 

$$h = \kappa \sigma_{\mathrm{Market}} S_0 \sqrt{\max(t_k,0.15)}N^{-0.2}$$

at discretization date $t_k$. Make sure to fine-tune the dimensionless bandwidth parameter $\kappa$. Its order of magnitude is 1.0. Use $\Delta t = 1/100$, $N=10,000$ paths. Note: In class, we described an acceleration technique that involves sorting the "particles" $(S_{t_k},a_{t_k})$ according to the spot value $S_{t_k}$. Since the kernel we have chosen has compact support and is fairly inexpensive to evaluate, you may ignore this acceleration technique here. This means that each estimation of a conditional expectation $\mathbb{E}\left[\left.a_{t_k}^2\right\vert S_{t_k}=x\right]$ (for $x$ in a grid of spot values) involves the ratio of two sums of $N$ terms each.

- Check that the resulting model is indeed calibrated to the market implied volatilities $\sigma_{\textrm{Market}} \equiv 15\%$. To this end, compute estimates of the call prices (maturity $T=1$) in the calibrated model for strikes $70, 80, 90, 100, 110, 120, 130, 140$, and invert the Black-Scholes formula to get the corresponding estimation of the implied volatilities $\hat\sigma(T,K)$. To estimate the call prices in the calibrated model, simulate a new set of independent paths with the calibrated leverage function $l$ and $N_2 = 100,000$ paths. For the inversion of the Black-Scholes formula, you can use the function <code>blackscholes_impv</code> provided below.

<b>(b).</b> Fix the spot-vol correlation $\rho = 0\%$ and mean reversion $\kappa=1$. We study the impact of volatility of volatility $\gamma$ on the smile in the pure stochastic volatility model and calibrated leverage function in the SLV model. Perform the following tasks with various values of $\gamma$. Suggested values of $\gamma$: $0\%$, $25\%$, $50\%$, $75\%$.
- Recalibrate the leverage function $l(t,S)$ for each $\gamma$, and plot the calibrated leverage function $l(t, S)$ as a function of the spot value $S$ for a fixed maturity, e.g., $t = T$ with various values of $\gamma$ in the same graph. Comment on the dependence of the shape of the leverage function on $\gamma$.
- Plot the corresponding smile at maturity $T$ for the pure stochastic volatility model (set the leverage function $l \equiv 1$) with the various values of $\gamma$ in the same graph. Comment on the dependence of the shape of the smile on $\gamma$.

<b>(c).</b> Fix the volatility of volatility $\gamma = 50\%$ and mean reversion $\kappa=1$. We study the impact of spot-vol correlation $\rho$ on the smile in the pure stochastic volatility model and calibrated leverage function in the SLV model. Perform the following tasks with various values of $\rho$. Suggested values of $\rho$: $-50\%$, $0\%$, $50\%$.
- Recalibrate the leverage function $l(t,S)$ for each $\rho$, and plot the calibrated leverage function $l(t, S)$ as a function of the spot value $S$ for a fixed maturity, e.g., $t = T$ with various values of $\rho$ in the same graph. Comment on the dependence of the shape of the leverage function on $\rho$.
- Plot the corresponding smile at maturity $T$ for the pure stochastic volatility model (set the leverage function $l \equiv 1$) with the various values of $\rho$ in the same graph. Comment on the dependence of the shape of the smile on $\rho$.

<b>(d).</b> Fix the spot-vol correlation $\rho = 0\%$ and volatility of volatility $\gamma = 50\%$. We study the impact of mean reversion $\kappa$ on the smile in the pure stochastic volatility model and calibrated leverage function in the SLV model. Perform the following tasks with various values of $\kappa$. Suggested values of $\kappa$: $0.1$, $1$, $10$.
- Recalibrate the leverage function $l(t,S)$ for each $\gamma$, and plot the calibrated leverage function $l(t, S)$ as a function of the spot value $S$ for a fixed maturity, e.g., $t = T$ with various values of $\gamma$ in the same graph. Comment on the dependence of the shape of the leverage function on $\kappa$.
- Plot the corresponding smile at maturity $T$ for the pure stochastic volatility model (set the leverage function $l \equiv 1$) with the various values of $\kappa$ in the same graph. Comment on the dependence of the shape of the smile on $\kappa$.

<b>(e).</b> Consider the forward-starting call spread with payoff
\begin{equation*}
\left( \frac{S_{T_2}}{S_{T_1}}-0.95 \right)_+ - \left( \frac{S_{T_2}}{S_{T_1}}-1.05 \right)_+
\end{equation*}
with $T_1 = T - \frac{3}{12}$, $T_2 = T$. Use $\gamma = 100\%$, $\rho = -50\%$ and $\kappa=1$. Compare the prices of this option in the Black-Scholes model with volatility 15% and in the calibrated SLV model. Comment on the result. Why is it of interest to use stochastic local volatility models for pricing derivatives?
"""



"""$$P_{\text{FS}} = e^{-rT} C_{\text{BS}}\left(1, K, T_2 - T_1, \sigma\right)$$
where, $(x, k, \tau, \sigma) \mapsto C_{\text{BS}}\left(x, k, \tau, \sigma\right)$ is the BS-pricing function of a call$(\texttt{strike} = k, \texttt{ttmatur.} = \tau)$
"""

def blackscholes_price(K, T, S, vol, r=0, q=0, callput='call'):
    """Compute the call/put option price in Black-Scholes model
    
    Parameters
    ----------
    K: scalar or array_like
        The strike of the option.
    T: scalar or array_like
        The maturity of the option.
    S: scalar or array_like
        The spot price of the underlying security.
    vol: scalar or array_like
        The implied Black-Scholes volatility.
    callput: str
        Must be either 'call' or 'put'

    Returns
    -------
    price: scalar or array_like
        The price of the option.

    Examples
    --------
    >>> blackscholes_price(95, 0.25, 100, 0.2, r=0.05, callput='put')
    1.5342604771222823
    """
    F = S*np.exp((r-q)*T)
    w = vol**2*T
    d1 = (np.log(F/K)+0.5*w)/np.sqrt(w)
    d2 = d1 - np.sqrt(w)
    callput = callput.lower()
    if callput == 'call':
        opttype = 1
    elif callput == 'put':
        opttype = -1
    else:
        raise ValueError('The value of callput must be either "call" or "put".')
    price = (opttype*F*norm.cdf(opttype*d1)-opttype*K*norm.cdf(opttype*(d2)))*np.exp(-r*T)
    return price

norm = stats.norm

# all inputs must be scalar
def blackscholes_impv_scalar(K, T, S, value, r=0, q=0, callput='call', tol=1e-6, maxiter=500):
    """Compute implied vol in Black-Scholes model
    
    Parameters
    ----------
    K: scalar
        The strike of the option.
    T: scalar
        The maturity of the option.
    S: scalar
        The spot price of the underlying security.
    value: scalar
        The value of the option
    callput: str
        Must be either 'call' or 'put'

    Returns
    -------
    vol: scalar
        The implied vol of the option.
    """
    if (K <= 0) or (T <= 0):
        return np.nan
    F = S*np.exp((r-q)*T)
    K = K/F
    value = value*np.exp(r*T)/F
    callput = callput.lower()
    if callput not in ['call', 'put']:
        raise ValueError('The value of "callput" must be either "call" or "put"')
    opttype = 1 if callput == 'call' else -1
    value -= max(opttype * (1 - K), 0)
    if value < 0:
        return np.nan
    if (value == 0):
        return 0
    j = 1
    p = np.log(K)
    if K >= 1:
        x0 = np.sqrt(2 * p)
        x1 = x0 - (0.5 - K * norm.cdf(-x0) - value) * np.sqrt(2*np.pi)
        while (abs(x0 - x1) > tol*np.sqrt(T)) and (j < maxiter):
            x0 = x1
            d1 = -p/x1+0.5*x1
            x1 = x1 - (norm.cdf(d1) - K*norm.cdf(d1-x1)-value)*np.sqrt(2*np.pi)*np.exp(0.5*d1**2)
            j += 1
        return x1 / np.sqrt(T)
    else:
        x0 = np.sqrt(-2 * p)
        x1 = x0 - (0.5*K-norm.cdf(-x0)-value)*np.sqrt(2*np.pi)/K
        while (abs(x0-x1) > tol*np.sqrt(T)) and (j < maxiter):
            x0 = x1
            d1 = -p/x1+0.5*x1
            x1 = x1-(K*norm.cdf(x1-d1)-norm.cdf(-d1)-value)*np.sqrt(2*np.pi)*np.exp(0.5*d1**2)
            j += 1
        return x1/np.sqrt(T)

# vectorized version
blackscholes_impv = np.vectorize(blackscholes_impv_scalar, excluded={'callput', 'tol', 'maxiter'})

# Example
blackscholes_impv(K=95, T=0.25, S=100, value=7, callput='call')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# impvs = np.array([blackscholes_impv(k, T = 1., S = s_0, value = get_price(k)) for k in strikes])

blackscholes_impv(strikes[0], T = 1., S = s_0, value = 5.978528810578943)

blackscholes_price(70., 1., s_0, .15)

def quartic_kernel(x):
    x = np.clip(x, -1, 1)
    return (x+1)**2*(1-x)**2